{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "tF42HvI7-gs5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiran703/A2C_demo/blob/main/A2C_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "TgZUkjKYSgvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install panda_gym"
      ],
      "metadata": {
        "id": "ABneW6tOSpyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "import torch as th\n",
        "\n",
        "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Dict):\n",
        "        # We do not know features-dim here before going over all the items,\n",
        "        # so put something dummy for now. PyTorch requires calling\n",
        "        # nn.Module.__init__ before adding modules\n",
        "        super().__init__(observation_space, features_dim=1)\n",
        "\n",
        "        extractors = {}\n",
        "\n",
        "        total_concat_size = 0\n",
        "        # We need to know size of the output of this extractor,\n",
        "        # so go over all the spaces and compute output feature sizes\n",
        "        for key, subspace in observation_space.spaces.items():\n",
        "                # Run through a simple MLP\n",
        "                extractors[key] = nn.Linear(subspace.shape[0], 16)\n",
        "                total_concat_size += 16\n",
        "\n",
        "        self.extractors = nn.ModuleDict(extractors)\n",
        "\n",
        "        # Update the features dim manually\n",
        "        self._features_dim = total_concat_size\n",
        "\n",
        "    def forward(self, observations) -> th.Tensor:\n",
        "        encoded_tensor_list = []\n",
        "\n",
        "        # self.extractors contain nn.Modules that do all the processing.\n",
        "        for key, extractor in self.extractors.items():\n",
        "            print(key)\n",
        "            print(extractor)\n",
        "            encoded_tensor_list.append(extractor)\n",
        "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
        "        return th.cat(encoded_tensor_list, dim=1)"
      ],
      "metadata": {
        "id": "tmdrBlrJ8iZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import panda_gym\n",
        "\n",
        "env = gym.make('PandaReach-v3')\n",
        "\n",
        "print(env.observation_space)\n",
        "\n",
        "temp = CustomCombinedExtractor(env.observation_space)\n",
        "\n",
        "print(temp)\n",
        "\n",
        "print(temp.forward(env.observation_space))\n",
        "\n"
      ],
      "metadata": {
        "id": "Pt_mm04k5loO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8a0K5jR836C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import gymnasium as gym\n",
        "import panda_gym\n",
        "\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n"
      ],
      "metadata": {
        "id": "HpiB8VdnQ7Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IOW9o1CHp7Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter = {\n",
        "    'lr' : 3e-4,\n",
        "    'hidden_size': 256,\n",
        "    'gamma': 0.99,\n",
        "    'n_steps': 100,\n",
        "    'max_episodes': 100\n",
        "}"
      ],
      "metadata": {
        "id": "wRCozRolTvu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# hyperparameters\n",
        "hidden_size = 24\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# Constants\n",
        "GAMMA = 0.99\n",
        "num_steps = 100\n",
        "max_episodes = 100"
      ],
      "metadata": {
        "id": "ULyU8KD8wqGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic_1(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "        value = F.relu(self.critic_linear1(state))\n",
        "        value = self.critic_linear2(value)\n",
        "\n",
        "        policy_dist = F.relu(self.actor_linear1(state))\n",
        "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
        "\n",
        "        return value, policy_dist"
      ],
      "metadata": {
        "id": "SeKdu_MtwqMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def a2c_1(env):\n",
        "    num_inputs = env.observation_space['observation'].shape[0]\n",
        "    num_outputs = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "    #print(num_inputs)\n",
        "    #print(num_outputs)\n",
        "\n",
        "    actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
        "    ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    all_lengths = []\n",
        "    average_lengths = []\n",
        "    all_rewards = []\n",
        "    entropy_term = 0\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        state = env.reset()\n",
        "        state = state['observation']\n",
        "        #state = np.array(state).flatten()\n",
        "        print(state.shape)\n",
        "        print(type(state))\n",
        "        for steps in range(num_steps):\n",
        "\n",
        "            value, policy_dist = actor_critic.forward(state)\n",
        "            value = value.detach().numpy()[0,0]\n",
        "            dist = policy_dist.detach().numpy()\n",
        "\n",
        "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
        "            log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
        "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "            log_probs.append(log_prob)\n",
        "            entropy_term += entropy\n",
        "            state = new_state\n",
        "\n",
        "            if done or steps == num_steps-1:\n",
        "                Qval, _ = actor_critic.forward(new_state)\n",
        "                Qval = Qval.detach().numpy()[0,0]\n",
        "                all_rewards.append(np.sum(rewards))\n",
        "                all_lengths.append(steps)\n",
        "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
        "                if episode % 10 == 0:\n",
        "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
        "                break\n",
        "\n",
        "        # compute Q values\n",
        "        Qvals = np.zeros_like(values)\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            Qval = rewards[t] + GAMMA * Qval\n",
        "            Qvals[t] = Qval\n",
        "\n",
        "        #update actor critic\n",
        "        values = torch.FloatTensor(values)\n",
        "        Qvals = torch.FloatTensor(Qvals)\n",
        "        log_probs = torch.stack(log_probs)\n",
        "\n",
        "        advantage = Qvals - values\n",
        "        actor_loss = (-log_probs * advantage).mean()\n",
        "        critic_loss = 0.5 * advantage.pow(2).mean()\n",
        "        ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
        "\n",
        "        ac_optimizer.zero_grad()\n",
        "        ac_loss.backward()\n",
        "        ac_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Plot results\n",
        "    smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
        "    smoothed_rewards = [elem for elem in smoothed_rewards]\n",
        "    plt.plot(all_rewards)\n",
        "    plt.plot(smoothed_rewards)\n",
        "    plt.plot()\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(all_lengths)\n",
        "    plt.plot(average_lengths)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Episode length')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "cEjoSBdlwsYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"PandaReachDense-v3\"\n",
        "env = make_vec_env(env_id, n_envs=4)\n",
        "\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
        "#env = gym.make(\"CartPole-v0\")\n",
        "a2c_1(env)"
      ],
      "metadata": {
        "id": "WB9svt3FrZQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"PandaReachDense-v3\"\n",
        "env = make_vec_env(env_id, n_envs=4)\n",
        "\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
      ],
      "metadata": {
        "id": "CrtZq6S1lhu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = A2C(policy = \"MultiInputPolicy\",\n",
        "            env = env,\n",
        "            verbose=1)"
      ],
      "metadata": {
        "id": "FKFLY54T-pU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(1_000_000)"
      ],
      "metadata": {
        "id": "4TuGHZD7RF1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and  VecNormalize statistics when saving the agent\n",
        "model.save(\"a2c-PandaReachDense-v3\")\n",
        "env.save(\"vec_normalize.pkl\")"
      ],
      "metadata": {
        "id": "MfYtjj19cKFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "# Load the saved statistics\n",
        "eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n",
        "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
        "\n",
        "# We need to override the render_mode\n",
        "eval_env.render_mode = \"rgb_array\"\n",
        "\n",
        "#  do not update them at test time\n",
        "eval_env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "eval_env.norm_reward = False\n",
        "\n",
        "# Load the agent\n",
        "model = A2C.load(\"a2c-PandaReachDense-v3\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "liirTVoDkHq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}